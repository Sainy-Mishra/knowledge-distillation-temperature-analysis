# ğŸ“˜ Knowledge Distillation Temperature Analysis

This project explores the effect of **temperature scaling** in **knowledge distillation** using the CIFAR-10 dataset. A large **Teacher model** is used to train a smaller, faster **Student model**. By varying the **temperature parameter**, we analyze how "softening" the output predictions affects the studentâ€™s learning performance.

---

## ğŸ¯ Project Goals

- Implement a teacher-student knowledge distillation framework.
- Run experiments with different temperature values.
- Evaluate the impact of temperature on student accuracy.
- Visualize and compare results.

---

## ğŸ§  What is Knowledge Distillation?

**Knowledge Distillation** is a model compression technique where a small model (Student) learns to mimic a larger, well-trained model (Teacher). Instead of learning just from ground-truth labels, the student also learns from the **soft predictions** of the teacher, which include richer information about class relationships.

The **temperature** controls how soft the teacherâ€™s predictions are. Higher temperatures spread the probabilities more evenly across classes, helping the student learn more subtle patterns.

---

## ğŸ—ï¸ Project Structure

```bash
project/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ models.py
â”œâ”€â”€ train.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ–¥ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/Sainy-Mishra/knowledge-distillation-temperature-analysis.git
cd knowledge-distillation-temperature-analysis

```
### 2. Install requirements

```bash
pip install -r requirements.txt
```

### 3. How to Run

```bash
python main.py --mode teacher --epochs 20
ğŸ‘¶ Train Student without Distillation
```
```bash
python main.py --mode student --epochs 20
ğŸ“š Train Student with Distillation
```

```bash
python main.py --mode distill --temperature 4 --alpha 0.3 --epochs 20
ğŸ“Š Run Temperature Experiment
```

```bash
python experiment_temperatur.py
âœ¨ Running the app
```

This will:

Run knowledge distillation with multiple temperature values

Save results to temperature_results.json

Generate a graph temperature_analysis.png
