# knowledge-distillation-temperature-analysis
This project explores the impact of temperature scaling in knowledge distillation using CIFAR-10. A lightweight student model is trained to mimic a larger teacher model across varying temperature values to find the optimal distillation setting. Includes automated experiments, accuracy plots, and model comparisons.
